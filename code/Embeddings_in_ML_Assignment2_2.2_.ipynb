{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.2\n",
    "\n",
    "## Goal\n",
    "\n",
    "The performance of machine learning systems directly depends on the quality of input features. In this exercise, you will investigate the impact of individual features on a system for named entity recognition: what does the inclusion of each individual feature do to the results? And what happens when they are combined?\n",
    "\n",
    "We saw in the lecture of Week 5 that the possibility of creating high quality word embeddings from large volumes of unlabelled data has improved results for many NLP tasks. In the second part of this exercise, we will illustrate this by comparing systems that use \\`traditional\\' feature values to word embeddings. \n",
    "\n",
    "\n",
    "## Acknowledgement\n",
    "\n",
    "This exercise made use of examples from the following exercise (in the HLT course):\n",
    "\n",
    "https://github.com/cltl/ma-hlt-labs/\n",
    "\n",
    "Lab3: machine learning\n",
    "\n",
    "\n",
    "## Procedure\n",
    "\n",
    "This notebook will provide the code for running the experiments outlined above. You will only need to make minor adaptations to run the feature ablation analysis. As specified in Assignment 2, you will need to write a short report on your findings. The report should include tables presenting results accompanied by an explanation. You may limit the report to a description of the features (and why they may matter), the results and a brief conclusion (i.e. introduction, related work, methodological setup, explanation of the machine learning models chosen are not needed).\n",
    "\n",
    "The notebooks and set up have been designed for educational purposes: design choices are based on clearly illustrating what is going on and facilitating the exercises.\n",
    "\n",
    "## The Data\n",
    "\n",
    "We will be working with the data from the CoNLL Named Entity Recognition shared task 2003. The data has been preprocessed to make some useful features directly availabe. It can be found in the Assignment 2.2.2 folder under data/.\n",
    "\n",
    "We will be using the file conll_train_prep.txt for training and conll_test_prep.txt for testing.\n",
    "\n",
    "The original data had the following (space separated) format:\n",
    "\n",
    "Token POS-tag Chunklabel Goldlabel\n",
    "\n",
    "The first lines of the conll file look like this:\n",
    "\n",
    "-DOCSTART- -X- -X- O\n",
    "  \n",
    "EU NNP B-NP B-ORG\n",
    "\n",
    "rejects VBZ B-VP O\n",
    "\n",
    "\n",
    "A note on Chunklabel:\n",
    "Chunks are phrases (or constituents), but unlike a full constituent parses that aim to provide the full sentence structure in constituents, chunkers provide shallow representations of a depth of maximal two. They typically return noun phrases, prepositional phrases and some times verb phrases (with no or few arguments). The labels indicate the type of phrase.\n",
    "\n",
    "A note on the Goldlabel:\n",
    "This is the correct interpretation according to human annotators. CoNLL follows the BIO labeling style for named entities as also illustrated in the Eisenstein chapter you read in Module 1.\n",
    "\n",
    "During preprocessing, we added two columns and used tsv to represent the conll file. The format is:\n",
    "\n",
    "Token  Preceding_token  Capitalization  POS-tag  Chunklabel  Goldlabel\n",
    "\n",
    "The first lines look like this:\n",
    "\n",
    "-DOCSTART-      FULLCAP -X-     -X-     O\n",
    "  \n",
    "EU              FULLCAP NNP     B-NP    B-ORG\n",
    "\n",
    "rejects EU      LOWCASE VBZ     B-VP    O\n",
    "\n",
    "Preceding_token: \n",
    "This column provides the token preceding the current token. (This is an empty space if there is no previous token).\n",
    "\n",
    "Capitalization: \n",
    "This column provides information on the capitalization of the token.\n",
    "It can take the following values:\n",
    "\n",
    "* LOWCASE: token consists of lowercase letters only\n",
    "* FULLCAP: token consists of uppercase letters only\n",
    "* FIRSTCAP: the first letter of the token is in uppercase, the rest in lowercase\n",
    "* NUMERIC: the token represents a numeric value\n",
    "* PUNCT: the token is a punctuation marker\n",
    "* MIXED: the token is a mixture of the above (all other)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "We will make use of the following packages:\n",
    "\n",
    "* scikit-learn : provides lots of useful implementations for machine learning (and has relatively good documentation!)\n",
    "* csv: a light-weight package to deal with data represented in csv (or related formats such as tsv)\n",
    "* gensim: a useful package for working with word embeddings\n",
    "* numpy: a packages that (among others) provides useful datastructures and operations to work with vectors\n",
    "\n",
    "Some notes on design decisions (feel free to ignore these if this is all new to you):\n",
    "\n",
    "* We are using csv rather than (the more common) pandas for working with the conll files, because pandas standardly applies type conversion, which we do not want when dealing with text that contains numbers (fixing this will make the code look more complex).\n",
    "* scikit-learn provides several machine learning algorithms, but this is not the focus of this exercise. We are using logistic regression, because it serves the purpose of our experiments and is relatively efficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell imports all the modules we'll need. Make sure to run this once before running the other cells\n",
    "\n",
    "\n",
    "#sklearn is scikit-learn\n",
    "import sklearn\n",
    "import csv\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Traditional Features\n",
    "\n",
    "In this first part, we will explore the impact of various features on named entity recognition.\n",
    "We will use so-called traditional features, where the feature values (strings) are presented by one-hot encoding \n",
    "\n",
    "## Step 1: A Basic Classifier\n",
    "\n",
    "We will first walk through the process of creating and evaluating a simple classifier that only uses the token itself as a feature. In the next step, we will run evaluations on this basic system.\n",
    "\n",
    "This is generally a good way to start experimenting: first walk through the entire experimental process with a very basic, easy to create system to see if everything works, there are no problems with the data etc. You can then build up from there towards a more sophististicated system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for feature extraction and training a classifier\n",
    "\n",
    "\n",
    "## For documentation on how to create input representations of features in scikit-learn:\n",
    "# https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "#Setting some variables that we will use multiple times\n",
    "trainfile = '/Users/hernando/Desktop/NLP/port/assignment2/NLP_tech_distributional_semantics 2/data/conll_train_cap.txt'\n",
    "testfile = '/Users/hernando/Desktop/NLP/port/assignment2/NLP_tech_distributional_semantics 2/data/conll_test_cap.txt'\n",
    "\n",
    "def extract_features_token_only_and_labels(conllfile):\n",
    "    '''Function that extracts features and gold label from preprocessed conll (here: tokens only).\n",
    "    \n",
    "    :param conllfile: path to the (preprocessed) conll file\n",
    "    :type conllfile: string\n",
    "    \n",
    "    \n",
    "    :return features: a list of dictionaries, with key-value pair providing the value for the feature `token' for individual instances\n",
    "    :return labels: a list of gold labels of individual instances\n",
    "    '''\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    conllinput = open(conllfile, 'r')\n",
    "    #delimiter indicates we are working with a tab separated value (default is comma)\n",
    "    #quotechar has as default value '\"', which is used to indicate the borders of a cell containing longer pieces of text\n",
    "    #in this file, we have only one token as text, but this token can be '\"', which then messes up the format. We set quotechar to a character that does not occur in our file\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        #I preprocessed the file so that all rows with instances should contain 6 values, the others are empty lines indicating the beginning of a sentence\n",
    "        if len(row) == 6:\n",
    "            #structuring feature value pairs as key-value pairs in a dictionary\n",
    "            #the first column in the conll file represents tokens\n",
    "            feature_value = {'Token': row[0]}\n",
    "            features.append(feature_value)\n",
    "            #The last column provides the gold label (= the correct answer). \n",
    "            labels.append(row[-1])\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "\n",
    "def create_vectorizer_and_classifier(features, labels):\n",
    "    '''\n",
    "    Function that takes feature-value pairs and gold labels as input and trains a logistic regression classifier\n",
    "    \n",
    "    :param features: feature-value pairs\n",
    "    :param labels: gold labels\n",
    "    :type features: a list of dictionaries\n",
    "    :type labels: a list of strings\n",
    "    \n",
    "    :return lr_classifier: a trained LogisticRegression classifier\n",
    "    :return vec: a DictVectorizer to which the feature values are fitted. \n",
    "    '''\n",
    "    \n",
    "    vec = DictVectorizer()\n",
    "    #fit creates a mapping between observed feature values and dimensions in a one-hot vector, transform represents the current values as a vector \n",
    "    tokens_vectorized = vec.fit_transform(features)\n",
    "    lr_classifier = LogisticRegression(solver='saga')\n",
    "    lr_classifier.fit(tokens_vectorized, labels)\n",
    "    \n",
    "    return lr_classifier, vec\n",
    "\n",
    "#extract features and labels:\n",
    "feature_values, labels = extract_features_token_only_and_labels(trainfile) \n",
    "#create vectorizer and trained classifier:\n",
    "lr_classifier, vectorizer = create_vectorizer_and_classifier(feature_values, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Evaluation\n",
    "\n",
    "We will now run a basic evaluation of the system on a test file. \n",
    "Two important properties of the test file:\n",
    "\n",
    "1. the test file and training file are independent sets (if they contain identical examples, this is coincidental)\n",
    "2. the test file is preprocessed in the exact same way as the training file \n",
    "\n",
    "The first function runs our classifier on the test data.\n",
    "\n",
    "The second function prints out a confusion matrix (comparing predictions and gold labels per class). \n",
    "You can find more information on confusion matrices here: https://www.geeksforgeeks.org/confusion-matrix-machine-learning/\n",
    "\n",
    "The third function prints out the macro precision, recall and f-score of the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1173      12     79      2      9       2      8      3    380\n",
      "B-MISC        10     390     15      1      2       8      1      2    273\n",
      "B-ORG        115      36    681      8     10       8     44      2    757\n",
      "B-PER         12       9     15    539      2       0      1     38   1001\n",
      "I-LOC         19       4      1      1     95       4     24      1    108\n",
      "I-MISC         0      11      0      0      1     117      3      0     84\n",
      "I-ORG         41      10     38      1     34       5    280      3    423\n",
      "I-PER          9       3     13     53      2       0      2    100    974\n",
      "O              7      28      9      0      5      13     43      5  38444\n",
      "P: 0.7662832549510706 R: 0.48137979346502646 F1: 0.564331316735006\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_predicted_and_gold_labels_token_only(testfile, vectorizer, classifier):\n",
    "    '''\n",
    "    Function that extracts features and runs classifier on a test file returning predicted and gold labels\n",
    "    \n",
    "    :param testfile: path to the (preprocessed) test file\n",
    "    :param vectorizer: vectorizer in which the mapping between feature values and dimensions is stored\n",
    "    :param classifier: the trained classifier\n",
    "    :type testfile: string\n",
    "    :type vectorizer: DictVectorizer\n",
    "    :type classifier: LogisticRegression()\n",
    "    \n",
    "    \n",
    "    \n",
    "    :return predictions: list of output labels provided by the classifier on the test file\n",
    "    :return goldlabels: list of gold labels as included in the test file\n",
    "    '''\n",
    "    \n",
    "    #we use the same function as above (guarantees features have the same name and form)\n",
    "    sparse_feature_reps, goldlabels = extract_features_token_only_and_labels(testfile)\n",
    "    #we need to use the same fitting as before, so now we only transform the current features according to this mapping (using only transform)\n",
    "    test_features_vectorized = vectorizer.transform(sparse_feature_reps)\n",
    "    predictions = classifier.predict(test_features_vectorized)\n",
    "    \n",
    "    return predictions, goldlabels\n",
    "\n",
    "def print_confusion_matrix(predictions, goldlabels):\n",
    "    '''\n",
    "    Function that prints out a confusion matrix\n",
    "    \n",
    "    :param predictions: predicted labels\n",
    "    :param goldlabels: gold standard labels\n",
    "    :type predictions, goldlabels: list of strings\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    #based on example from https://datatofish.com/confusion-matrix-python/ \n",
    "    data = {'Gold':    goldlabels, 'Predicted': predictions    }\n",
    "    df = pd.DataFrame(data, columns=['Gold','Predicted'])\n",
    "\n",
    "    confusion_matrix = pd.crosstab(df['Gold'], df['Predicted'], rownames=['Gold'], colnames=['Predicted'])\n",
    "    print (confusion_matrix)\n",
    "\n",
    "\n",
    "def print_precision_recall_fscore(predictions, goldlabels):\n",
    "    '''\n",
    "    Function that prints out precision, recall and f-score\n",
    "    \n",
    "    :param predictions: predicted output by classifier\n",
    "    :param goldlabels: original gold labels\n",
    "    :type predictions, goldlabels: list of strings\n",
    "    '''\n",
    "    \n",
    "    precision = metrics.precision_score(y_true=goldlabels,\n",
    "                        y_pred=predictions,\n",
    "                        average='macro')\n",
    "\n",
    "    recall = metrics.recall_score(y_true=goldlabels,\n",
    "                     y_pred=predictions,\n",
    "                     average='macro')\n",
    "\n",
    "\n",
    "    fscore = metrics.f1_score(y_true=goldlabels,\n",
    "                 y_pred=predictions,\n",
    "                 average='macro')\n",
    "\n",
    "    print('P:', precision, 'R:', recall, 'F1:', fscore)\n",
    "    \n",
    "#vectorizer and lr_classifier are the vectorizer and classifiers created in the previous cell.\n",
    "#it is important that the same vectorizer is used for both training and testing: they should use the same mapping from values to dimensions\n",
    "predictions, goldlabels = get_predicted_and_gold_labels_token_only(testfile, vectorizer, lr_classifier)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: A More Elaborate System\n",
    "\n",
    "Now that we have run a basic experiment, we are going to investigate alternatives. In this exercise, we only focus on features. We will continue to use the same logistic regression classifier throughout the exercise.\n",
    "\n",
    "We want to investigate the impact of individual features. We will thus use a function that allows us to specify whether we include a specific feature or not. The features we have at our disposal are:\n",
    "\n",
    "* the token itself (as used above)\n",
    "* the preceding token\n",
    "* the capitalization indication (see above for values that this takes)\n",
    "* the pos-tag of the token\n",
    "* the chunklabel of the chunk the token is part of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_index = {'Token': 0, 'Prevtoken': 1, 'Cap': 2, 'Pos': 3, 'Chunklabel': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the functions with multiple features and analysis\n",
    "\n",
    "#defines the column in which each feature is located (note: you can also define headers and use csv.DictReader)\n",
    "feature_to_index = {'Token': 0, 'Prevtoken': 1, 'Cap': 2, 'Pos': 3, 'Chunklabel': 4}\n",
    "\n",
    "\n",
    "def extract_features_and_gold_labels(conllfile, selected_features):\n",
    "    '''Function that extracts features and gold label from preprocessed conll (here: tokens only).\n",
    "    \n",
    "    :param conllfile: path to the (preprocessed) conll file\n",
    "    :type conllfile: string\n",
    "    \n",
    "    \n",
    "    :return features: a list of dictionaries, with key-value pair providing the value for the feature `token' for individual instances\n",
    "    :return labels: a list of gold labels of individual instances\n",
    "    '''\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    conllinput = open(conllfile, 'r')\n",
    "    #delimiter indicates we are working with a tab separated value (default is comma)\n",
    "    #quotechar has as default value '\"', which is used to indicate the borders of a cell containing longer pieces of text\n",
    "    #in this file, we have only one token as text, but this token can be '\"', which then messes up the format. We set quotechar to a character that does not occur in our file\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        #I preprocessed the file so that all rows with instances should contain 6 values, the others are empty lines indicating the beginning of a sentence\n",
    "        if len(row) == 6:\n",
    "            #structuring feature value pairs as key-value pairs in a dictionary\n",
    "            #the first column in the conll file represents tokens\n",
    "            feature_value = {}\n",
    "            for feature_name in selected_features:\n",
    "                row_index = feature_to_index.get(feature_name)\n",
    "                feature_value[feature_name] = row[row_index]\n",
    "            features.append(feature_value)\n",
    "            #The last column provides the gold label (= the correct answer). \n",
    "            labels.append(row[-1])\n",
    "    return features, labels\n",
    "\n",
    "def get_predicted_and_gold_labels(testfile, vectorizer, classifier, selected_features):\n",
    "    '''\n",
    "    Function that extracts features and runs classifier on a test file returning predicted and gold labels\n",
    "    \n",
    "    :param testfile: path to the (preprocessed) test file\n",
    "    :param vectorizer: vectorizer in which the mapping between feature values and dimensions is stored\n",
    "    :param classifier: the trained classifier\n",
    "    :type testfile: string\n",
    "    :type vectorizer: DictVectorizer\n",
    "    :type classifier: LogisticRegression()\n",
    "    \n",
    "    \n",
    "    \n",
    "    :return predictions: list of output labels provided by the classifier on the test file\n",
    "    :return goldlabels: list of gold labels as included in the test file\n",
    "    '''\n",
    "    \n",
    "    #we use the same function as above (guarantees features have the same name and form)\n",
    "    features, goldlabels = extract_features_and_gold_labels(testfile, selected_features)\n",
    "    #we need to use the same fitting as before, so now we only transform the current features according to this mapping (using only transform)\n",
    "    test_features_vectorized = vectorizer.transform(features)\n",
    "    predictions = classifier.predict(test_features_vectorized)\n",
    "    \n",
    "    return predictions, goldlabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1348      18    134     70      1       1      7     46     43\n",
      "B-MISC        28     458     54     47      0       1      8     24     82\n",
      "B-ORG        184      43   1038    175      1       1     26     57    136\n",
      "B-PER         68      20     45   1258      0       0      1    141     84\n",
      "I-LOC          5       0      0      0    155       4     40     30     23\n",
      "I-MISC         1       8      0      3      3     128     12     23     38\n",
      "I-ORG         21       8      5     11     45       8    521    111    105\n",
      "I-PER          4       1      0     13      1       0     15   1093     29\n",
      "O             29      41     70    101      3      37     36    105  38132\n",
      "P: 0.775359110733929 R: 0.7352997074459117 F1: 0.7489500540371716\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "#define which from the available features will be used (names must match key names of dictionary feature_to_index)\n",
    "all_features = ['Token','Prevtoken','Cap','Pos','Chunklabel']\n",
    "\n",
    "sparse_feature_reps, labels = extract_features_and_gold_labels(trainfile, all_features)\n",
    "#we can use the same function as before for creating the classifier and vectorizer\n",
    "lr_classifier, vectorizer = create_vectorizer_and_classifier(sparse_feature_reps, labels)\n",
    "#when applying our model to new data, we need to use the same features\n",
    "predictions, goldlabels = get_predicted_and_gold_labels(testfile, vectorizer, lr_classifier, all_features)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Ablation Analysis\n",
    "\n",
    "If all worked well, the system that made use of all features worked better than the system with just the tokens.\n",
    "We now want to know which of the features contributed to this improved: do we want to include all features?\n",
    "Or just some?\n",
    "\n",
    "We can investigate this using *feature ablation analysis*. This means that we systematically test what happens if we add or remove a specific feature. Ideally, we investigate all possible combinations.\n",
    "\n",
    "The cell below illustrates how you can use the code above to investigate a system with three features. You can modify the selected features to try out different combinations. You can either do this manually and rerun the cell or write a function that creates list of all combinations you want to tests and runs them one after the other.\n",
    "\n",
    "Include your results in the report of this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Token', 'Prevtoken'),\n",
       " ('Token', 'Cap'),\n",
       " ('Token', 'Pos'),\n",
       " ('Token', 'Chunklabel'),\n",
       " ('Prevtoken', 'Cap'),\n",
       " ('Prevtoken', 'Pos'),\n",
       " ('Prevtoken', 'Chunklabel'),\n",
       " ('Cap', 'Pos'),\n",
       " ('Cap', 'Chunklabel'),\n",
       " ('Pos', 'Chunklabel')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1231      13     54     10      2       1      3      0    354\n",
      "B-MISC        11     392     18      3      0       0      1      0    277\n",
      "B-ORG        102      36    785     17      0       0     14      0    707\n",
      "B-PER         10       8     12    753      0       0      0      5    829\n",
      "I-LOC          7       0      1      0    147       5     24      3     70\n",
      "I-MISC         0       8      0      1      1     123      7      1     75\n",
      "I-ORG         13       9      2      1     43       6    417      2    342\n",
      "I-PER          3       0      5      7      1       0      5    462    673\n",
      "O             12      29     30      4      3      16     20      3  38437\n",
      "P: 0.8656238604796762 R: 0.5857941718382637 F1: 0.6851065088645175\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1277      13     85    147      9       2     15      5    115\n",
      "B-MISC        14     430     35    106      2      10      2      5     98\n",
      "B-ORG        163      36    870    253     10       8     57      3    261\n",
      "B-PER         16       9     16   1397      2       0      2    117     58\n",
      "I-LOC         23       4      1     39    123       4     27      1     35\n",
      "I-MISC         0      11      1     33      2     121      3      0     45\n",
      "I-ORG         45      11     65    123     39       8    380      5    159\n",
      "I-PER         11       3     14    859      2       0      7    210     50\n",
      "O             10      32     27    312      6      15     54      7  38091\n",
      "P: 0.7144646676342121 R: 0.6032639525096201 F1: 0.6293223226811653\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1263      12     82    212      9       1     14      5     70\n",
      "B-MISC        13     413     18    108      2       7     16      5    120\n",
      "B-ORG        178      28    796    377     10       8     70      2    192\n",
      "B-PER         16      11     15   1354      2       0      7    104    108\n",
      "I-LOC         26       1      2     49    125       3     31      1     19\n",
      "I-MISC         0      12      1     34      2     122      5      0     40\n",
      "I-ORG         44       8     73    125     39       8    399     12    127\n",
      "I-PER         11       3     14    865      2       0     16    218     27\n",
      "O              9      30     26    419      4      48     52     14  37952\n",
      "P: 0.6960280728975197 R: 0.5960114596754713 F1: 0.6159219850224483\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1202      14     71      2      2       2      9      3    363\n",
      "B-MISC        14     398     15      2      0       6      1      1    265\n",
      "B-ORG        122      38    762     10      5       8     27      1    688\n",
      "B-PER         10       9     15    529      0       0      0     33   1021\n",
      "I-LOC          9       4      3      0    107       4     25      1    104\n",
      "I-MISC         0      11      0      0      1     119      3      0     82\n",
      "I-ORG         30      12     32      0     41       5    303     11    401\n",
      "I-PER          5       3      8     22      2       0      7    168    941\n",
      "O              8      30     15      0      4      14     17      2  38464\n",
      "P: 0.805335655012509 R: 0.5051800904794447 F1: 0.5940991861798264\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC        743      76     54    224      0       1      3     97    470\n",
      "B-MISC        69     187     26     93      0       1      4     74    248\n",
      "B-ORG        198     120    289    114      0       0      3     92    845\n",
      "B-PER        123      17     42    901      0       1      0    157    376\n",
      "I-LOC          4       0      0      0    136       6     42     41     28\n",
      "I-MISC         1       2      0      4      6      80     21     40     62\n",
      "I-ORG         30       4      2     23     53      15    431    160    117\n",
      "I-PER          0       0      0      2      1       0     15   1090     48\n",
      "O            157      37     40    110      7      42     52    145  37964\n",
      "P: 0.636258509111638 R: 0.5318162178447188 F1: 0.5561544725851841\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC        735      13    439    241      0       1      3     85    151\n",
      "B-MISC        26      33    116     73      0       1      7     45    401\n",
      "B-ORG        180       6    931    149      0       0      6     97    292\n",
      "B-PER        112       2    283    935      0       2      2    155    126\n",
      "I-LOC          4       0      0      0    131       6     45     51     20\n",
      "I-MISC         1       0      0      3      3      83     23     49     54\n",
      "I-ORG         20       8      4     22     51      15    426    164    125\n",
      "I-PER          0       0      0      2      1       0     19   1109     25\n",
      "O            145      12    339    115      7      38     68    335  37495\n",
      "P: 0.6170088723804036 R: 0.5513815734410382 F1: 0.5520477736784799\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC        381       0      1     16      0       0      1      0   1269\n",
      "B-MISC         5       2      0      1      0       1      0      0    693\n",
      "B-ORG         82       0     25     11      0       0      0      0   1543\n",
      "B-PER         52       0      7    439      0       0      0      3   1116\n",
      "I-LOC          0       0      0      0    119       2     29      4    103\n",
      "I-MISC         0       0      0      2      6      66      5      0    137\n",
      "I-ORG          1       0      0      7     51       3    236      5    532\n",
      "I-PER          0       0      0      0      1       0     11    469    675\n",
      "O            181       4      4     42     31      51     55     19  38167\n",
      "P: 0.6676728268462414 R: 0.32941162628027254 F1: 0.39558344654635985\n",
      "Predicted  B-MISC  B-ORG  B-PER  I-ORG      O\n",
      "Gold                                         \n",
      "B-LOC          28      0   1238      3    399\n",
      "B-MISC        275      1    237     31    158\n",
      "B-ORG          54      9    969     24    605\n",
      "B-PER          22      6   1427      6    156\n",
      "I-LOC          12      0    173      8     64\n",
      "I-MISC          5      0    135      5     71\n",
      "I-ORG           3      1    627     42    162\n",
      "I-PER           0      0   1079     12     65\n",
      "O             102      1    929      9  37513\n",
      "P: 0.27949546411272036 R: 0.25588367556870667 F1: 0.20639770751494935\n",
      "Predicted  B-PER  I-PER      O\n",
      "Gold                          \n",
      "B-LOC          1    229   1438\n",
      "B-MISC         1    308    393\n",
      "B-ORG          2    363   1296\n",
      "B-PER          4    624    989\n",
      "I-LOC          0    183     74\n",
      "I-MISC         0    146     70\n",
      "I-ORG          0    632    203\n",
      "I-PER          0   1085     71\n",
      "O              5    290  38259\n",
      "P: 0.16475872028572466 R: 0.2148226028997825 F1: 0.1531289588389754\n",
      "Predicted  B-LOC  B-MISC  B-ORG  I-ORG  I-PER      O\n",
      "Gold                                                \n",
      "B-LOC       1255       1      0      2    286    124\n",
      "B-MISC       142       6      0     27    166    361\n",
      "B-ORG        955      15      2     10    441    238\n",
      "B-PER        883       4      0      2    620    108\n",
      "I-LOC          4       0      0     15    219     19\n",
      "I-MISC         3       0      0      7    161     45\n",
      "I-ORG         37       7      0     36    649    106\n",
      "I-PER         10       1      1     11   1109     24\n",
      "O            904       8      2      8    710  36922\n",
      "P: 0.26383759715177274 R: 0.3024750309891378 F1: 0.209959329478901\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample2=list(itertools.combinations(all_features,2))\n",
    "for k in sample2:\n",
    "    selected_features = list(k)\n",
    "    feature_values, labels = extract_features_and_gold_labels(trainfile, selected_features)\n",
    "    #we can use the same function as before for creating the classifier and vectorizer\n",
    "    lr_classifier, vectorizer = create_vectorizer_and_classifier(feature_values, labels)\n",
    "    #when applying our model to new data, we need to use the same features\n",
    "    predictions, goldlabels = get_predicted_and_gold_labels(testfile, vectorizer, lr_classifier, selected_features)\n",
    "    print_confusion_matrix(predictions, goldlabels)\n",
    "    print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Token', 'Prevtoken', 'Cap'),\n",
       " ('Token', 'Prevtoken', 'Pos'),\n",
       " ('Token', 'Prevtoken', 'Chunklabel'),\n",
       " ('Token', 'Cap', 'Pos'),\n",
       " ('Token', 'Cap', 'Chunklabel'),\n",
       " ('Token', 'Pos', 'Chunklabel'),\n",
       " ('Prevtoken', 'Cap', 'Pos'),\n",
       " ('Prevtoken', 'Cap', 'Chunklabel'),\n",
       " ('Prevtoken', 'Pos', 'Chunklabel'),\n",
       " ('Cap', 'Pos', 'Chunklabel')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1349      14     76     51      2       1      9     54    112\n",
      "B-MISC        23     436     51     37      0       0      3     35    117\n",
      "B-ORG        167      46    983     71      1       1     20     76    296\n",
      "B-PER         70      12     23   1152      0       0      1    151    208\n",
      "I-LOC          9       0      1      0    157       4     34     29     23\n",
      "I-MISC         1       9      0      2      3     128      9     25     39\n",
      "I-ORG         27       9      5     10     45       9    518    107    105\n",
      "I-PER          3       0      3     13      1       0     12   1093     31\n",
      "O             33      32     43     53      2      37     31    118  38205\n",
      "P: 0.7927099653832518 R: 0.7215971682331324 F1: 0.7475629935442316\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1334      18    132     54      2       1     11     52     64\n",
      "B-MISC        23     411     50     42      0       0      6     29    141\n",
      "B-ORG        179      31   1074     91      0       2     25     80    179\n",
      "B-PER         64       9    139   1139      0       0      2    145    119\n",
      "I-LOC          8       0      1      0    154       4     34     38     18\n",
      "I-MISC         1       9      0      1      2     128     10     26     39\n",
      "I-ORG         23       9      4     10     46       9    520    108    106\n",
      "I-PER          3       0      5     12      1       0     13   1096     26\n",
      "O             28      32    129     50      2      27     57    221  38008\n",
      "P: 0.7733911621073393 R: 0.7205248182098313 F1: 0.7379140812691705\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1246      13     62     12      1       1      5      1    327\n",
      "B-MISC        12     394     21      6      0       0      2      0    267\n",
      "B-ORG        101      35    809     19      1       0     17      0    679\n",
      "B-PER          9       9     13    788      0       0      0      6    792\n",
      "I-LOC          4       0      0      0    145       5     28      6     69\n",
      "I-MISC         0       7      0      1      1     123      8      1     75\n",
      "I-ORG         12       9      1      1     43       8    434      2    325\n",
      "I-PER          4       0      1      7      1       0      7    524    612\n",
      "O             12      29     37      4      2      16     21      7  38426\n",
      "P: 0.860746544317103 R: 0.5984453986648557 F1: 0.6952154837478282\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1278      17     83    139      9       2     13      4    123\n",
      "B-MISC        13     456     31     61      2       7     15      7    110\n",
      "B-ORG        179      35    873    208     10       8     68      3    277\n",
      "B-PER         16      28     17   1330      2       0      6    105    113\n",
      "I-LOC         26       2      2     33    125       3     30      1     35\n",
      "I-MISC         0      13      1     28      2     120      5      1     46\n",
      "I-ORG         46       8     71    109     39       8    406     10    138\n",
      "I-PER         11       3     14    846      2       0     15    218     47\n",
      "O             10      66     29    194      4      15     50     35  38151\n",
      "P: 0.7061479873373541 R: 0.6077944872481423 F1: 0.633192034870981\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1270      14     79    105      2       2     15     71    110\n",
      "B-MISC        15     411     24     52      0       8      3     88    101\n",
      "B-ORG        180      38    857    165      5       8     30    131    247\n",
      "B-PER         15       9     18   1124      0       0      0    385     66\n",
      "I-LOC         12       4      3      1    132       3     31     43     28\n",
      "I-MISC         0      11      1      3      2     121      5     30     43\n",
      "I-ORG         33      13     39     24     43       5    407    137    134\n",
      "I-PER          5       3      8     40      2       0     11   1045     42\n",
      "O             10      31     31    195      4      16     26    133  38108\n",
      "P: 0.750570882805997 R: 0.6679521874670316 F1: 0.6913903526376542\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1249      16     72    164      2       1     16     90     58\n",
      "B-MISC        14     394     21     63      0      11     20     63    116\n",
      "B-ORG        176      28    799    246      5       8     38    180    181\n",
      "B-PER         13      11     20   1082      0       0      1    387    103\n",
      "I-LOC         12       0      3      1    130       4     38     52     17\n",
      "I-MISC         0      11      0      3      2     123      5     34     38\n",
      "I-ORG         34       8     43     21     43       5    418    150    113\n",
      "I-PER          5       3     10     37      2       0     19   1054     26\n",
      "O              7      30     21    177      3      44     26    325  37921\n",
      "P: 0.7223626659556902 R: 0.6590507137140174 F1: 0.6709712249029628\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC        736      30    437    243      0       1      3    101    117\n",
      "B-MISC        44     307    125     51      0       1      8     37    129\n",
      "B-ORG        210      48    951    125      0       0      5     90    232\n",
      "B-PER        129      24    270    944      0       1      2    156     91\n",
      "I-LOC          4       7      0      0    130       7     41     38     30\n",
      "I-MISC         1       2      0      4      3      94     22     39     51\n",
      "I-ORG         23       9      5     22     51      16    445    150    114\n",
      "I-PER          0       0      0      2      1       0     20   1092     41\n",
      "O            142     106    279    121     11      39     46    139  37671\n",
      "P: 0.6454952404873835 R: 0.603400264805996 F1: 0.612676839412578\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC        758      77     48    205      0       1      2     65    512\n",
      "B-MISC        78     188     27     83      0       1      4     64    257\n",
      "B-ORG        206     122    284    106      0       0      3     73    867\n",
      "B-PER        130      18     26    881      0       0      0    155    407\n",
      "I-LOC          4       0      0      0    136       6     42     41     28\n",
      "I-MISC         1       1      0      4      6      80     21     40     63\n",
      "I-ORG         33       4      2     20     53      15    430    159    119\n",
      "I-PER          0       0      0      2      1       0     15   1079     59\n",
      "O            172      37     33    102      7      44     50    122  37987\n",
      "P: 0.6431642953507151 R: 0.5301408710353228 F1: 0.557798741331626\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC        751      13    437    219      0       1      3     61    183\n",
      "B-MISC        30      34    114     69      0       1      8     46    400\n",
      "B-ORG        189       6    929    141      0       0      6     73    317\n",
      "B-PER        120       3    285    898      0       1      1    154    155\n",
      "I-LOC          4       0      0      0    131       6     45     51     20\n",
      "I-MISC         1       0      0      3      3      83     23     50     53\n",
      "I-ORG         23       8      4     19     51      15    430    165    120\n",
      "I-PER          0       1      0      2      1       0     18   1099     35\n",
      "O            160      10    339     98      7      43     73    304  37520\n",
      "P: 0.617501602355182 R: 0.5495725950963208 F1: 0.552679357408721\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                               \n",
      "B-LOC       1266      27      1      0       0      2    221    151\n",
      "B-MISC       143     276      7      0       0     25    146    105\n",
      "B-ORG        937      52     25      0       0      9    334    304\n",
      "B-PER        877      18      5      0       0      2    616     99\n",
      "I-LOC          4      12      0      0       0      8    169     64\n",
      "I-MISC         5       5      0      0       0      5    133     68\n",
      "I-ORG         37       4      7      0       0     35    592    160\n",
      "I-PER         12       0      2      0       0     11   1099     32\n",
      "O            833      99     11      2       1      1    284  37323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 0.3262078243120412 R: 0.34754281555494837 F1: 0.27087557097750536\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample3=list(itertools.combinations(all_features,3))\n",
    "for k in sample3:\n",
    "    selected_features = list(k)\n",
    "    feature_values, labels = extract_features_and_gold_labels(trainfile, selected_features)\n",
    "    #we can use the same function as before for creating the classifier and vectorizer\n",
    "    lr_classifier, vectorizer = create_vectorizer_and_classifier(feature_values, labels)\n",
    "    #when applying our model to new data, we need to use the same features\n",
    "    predictions, goldlabels = get_predicted_and_gold_labels(testfile, vectorizer, lr_classifier, selected_features)\n",
    "    print_confusion_matrix(predictions, goldlabels)\n",
    "    print_precision_recall_fscore(predictions, goldlabels)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Token', 'Prevtoken', 'Cap', 'Pos'),\n",
       " ('Token', 'Prevtoken', 'Cap', 'Chunklabel'),\n",
       " ('Token', 'Prevtoken', 'Pos', 'Chunklabel'),\n",
       " ('Token', 'Cap', 'Pos', 'Chunklabel'),\n",
       " ('Prevtoken', 'Cap', 'Pos', 'Chunklabel')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1342      19    128     63      2       1      8     55     50\n",
      "B-MISC        28     464     48     43      0       1      6     21     91\n",
      "B-ORG        189      43   1045    154      1       1     23     68    137\n",
      "B-PER         64      18     35   1259      0       0      2    147     92\n",
      "I-LOC          9       0      1      0    155       4     34     28     26\n",
      "I-MISC         1       9      0      2      3     128     10     22     41\n",
      "I-ORG         26       8      5     11     44      10    517    109    105\n",
      "I-PER          3       0      2     13      1       0     17   1089     31\n",
      "O             33      37     70     88      3      38     32    111  38142\n",
      "P: 0.7783202427869254 R: 0.7354987512056013 F1: 0.7501770883126218\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1341      14     87     64      1       1      7     45    108\n",
      "B-MISC        23     439     66     44      0       2      3     32     93\n",
      "B-ORG        162      47   1072     92      1       1     23     66    197\n",
      "B-PER         75      12    160   1165      0       0      1    140     64\n",
      "I-LOC          5       0      0      0    158       4     36     31     23\n",
      "I-MISC         1       8      0      3      3     128     11     25     37\n",
      "I-ORG         24       7      5     11     46       9    511    116    106\n",
      "I-PER          4       0      0     14      1       0     12   1095     30\n",
      "O             29      32    178     62      2      40     37    107  38067\n",
      "P: 0.7726432385747475 R: 0.7276813488781277 F1: 0.7435491872967083\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1336      16    135     52      1       1      7     46     74\n",
      "B-MISC        23     411     50     43      0       0      7     28    140\n",
      "B-ORG        174      31   1073     96      1       2     26     64    194\n",
      "B-PER         68      10    139   1127      0       0      1    137    135\n",
      "I-LOC          5       0      0      0    154       4     36     41     17\n",
      "I-MISC         1       8      0      1      2     127     10     29     38\n",
      "I-ORG         21       9      3      7     46       9    511    123    106\n",
      "I-PER          4       0      0      4      1       0     15   1099     33\n",
      "O             28      34    128     50      2      24     50    243  37995\n",
      "P: 0.7754391305829205 R: 0.7183054556652402 F1: 0.7371827698611492\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1264      20     74    100      2       1     13     74    120\n",
      "B-MISC        18     436     26     38      0       8     18     54    104\n",
      "B-ORG        179      37    861    140      5       8     35    123    273\n",
      "B-PER         17      23     22   1063      0       0      1    382    109\n",
      "I-LOC         12       0      3      1    130       3     37     40     31\n",
      "I-MISC         0      12      1      3      2     124      6     27     41\n",
      "I-ORG         33       8     46     22     43       5    402    142    134\n",
      "I-PER          4       3     10     37      2       0     18   1065     17\n",
      "O             10      63     36    112      3      43     23    145  38119\n",
      "P: 0.7346075181416317 R: 0.6695526915230714 F1: 0.6872071624929891\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC        754      32    436    259      0       1      3     67    116\n",
      "B-MISC        48     309    124     45      0       2      8     40    126\n",
      "B-ORG        216      47    953    138      0       0      6     67    234\n",
      "B-PER        135      20    273    938      0       1      1    154     95\n",
      "I-LOC          4       7      0      0    131       7     42     38     28\n",
      "I-MISC         1       2      0      5      3      94     22     38     51\n",
      "I-ORG         26       9      4     24     51      16    443    148    114\n",
      "I-PER          0       1      0     11      1       0     18   1084     41\n",
      "O            157     108    283    129     11      44     50    110  37662\n",
      "P: 0.6429722617725466 R: 0.6040086954171593 F1: 0.6133073755525646\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "sample4=list(itertools.combinations(all_features,4))\n",
    "for k in sample4:\n",
    "    selected_features = list(k)\n",
    "    feature_values, labels = extract_features_and_gold_labels(trainfile, selected_features)\n",
    "    #we can use the same function as before for creating the classifier and vectorizer\n",
    "    lr_classifier, vectorizer = create_vectorizer_and_classifier(feature_values, labels)\n",
    "    #when applying our model to new data, we need to use the same features\n",
    "    predictions, goldlabels = get_predicted_and_gold_labels(testfile, vectorizer, lr_classifier, selected_features)\n",
    "    print_confusion_matrix(predictions, goldlabels)\n",
    "    print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Token',), ('Prevtoken',), ('Cap',), ('Pos',), ('Chunklabel',)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1173      12     79      2      9       2      8      3    380\n",
      "B-MISC        10     390     15      1      2       8      1      2    273\n",
      "B-ORG        115      36    681      8     10       8     44      2    757\n",
      "B-PER         12       9     15    539      2       0      1     38   1001\n",
      "I-LOC         19       4      1      1     95       4     24      1    108\n",
      "I-MISC         0      11      0      0      1     117      3      0     84\n",
      "I-ORG         41      10     38      1     34       5    280      3    423\n",
      "I-PER          9       3     13     53      2       0      2    100    974\n",
      "O              7      28      9      0      5      13     43      5  38444\n",
      "P: 0.7662832549510706 R: 0.48137979346502646 F1: 0.564331316735006\n",
      "Predicted  B-LOC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                              \n",
      "B-LOC        390      0     15      0       0      1      0   1262\n",
      "B-MISC         5      0      0      0       1      0      0    696\n",
      "B-ORG         84     18      9      0       0      0      0   1550\n",
      "B-PER         52      5    311      0       0      0      3   1246\n",
      "I-LOC          0      0      0    114       1     23      4    115\n",
      "I-MISC         0      0      2      4      66      4      0    140\n",
      "I-ORG          1      0      5     49       4    192      3    581\n",
      "I-PER          0      0      0      1       0      3    410    742\n",
      "O            219      1     19     45      59     36     46  38129\n",
      "P: 0.6276172904512252 R: 0.30663383284251267 F1: 0.36654893658946075\n",
      "Predicted      O\n",
      "Gold            \n",
      "B-LOC       1668\n",
      "B-MISC       702\n",
      "B-ORG       1661\n",
      "B-PER       1617\n",
      "I-LOC        257\n",
      "I-MISC       216\n",
      "I-ORG        835\n",
      "I-PER       1156\n",
      "O          38554\n",
      "P: 0.0917965494745163 R: 0.1111111111111111 F1: 0.10053456413465803\n",
      "Predicted  B-LOC  I-ORG      O\n",
      "Gold                          \n",
      "B-LOC       1556      3    109\n",
      "B-MISC       312     33    357\n",
      "B-ORG       1399     25    237\n",
      "B-PER       1505      6    106\n",
      "I-LOC        223     15     19\n",
      "I-MISC       166      7     43\n",
      "I-ORG        686     43    106\n",
      "I-PER       1122     12     22\n",
      "O           1626     16  36912\n",
      "P: 0.15815936608892323 R: 0.21575123427198012 F1: 0.15056874008897914\n",
      "Predicted      O\n",
      "Gold            \n",
      "B-LOC       1668\n",
      "B-MISC       702\n",
      "B-ORG       1661\n",
      "B-PER       1617\n",
      "I-LOC        257\n",
      "I-MISC       216\n",
      "I-ORG        835\n",
      "I-PER       1156\n",
      "O          38554\n",
      "P: 0.0917965494745163 R: 0.1111111111111111 F1: 0.10053456413465803\n"
     ]
    }
   ],
   "source": [
    "sample1=list(itertools.combinations(all_features,1))\n",
    "for k in sample1:\n",
    "    selected_features = list(k)\n",
    "    feature_values, labels = extract_features_and_gold_labels(trainfile, selected_features)\n",
    "    #we can use the same function as before for creating the classifier and vectorizer\n",
    "    lr_classifier, vectorizer = create_vectorizer_and_classifier(feature_values, labels)\n",
    "    #when applying our model to new data, we need to use the same features\n",
    "    predictions, goldlabels = get_predicted_and_gold_labels(testfile, vectorizer, lr_classifier, selected_features)\n",
    "    print_confusion_matrix(predictions, goldlabels)\n",
    "    print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: One-hot versus Embeddings\n",
    "\n",
    "In this second part of the exercise, we will compare results using one-hot encodings to pretrained word embeddings.\n",
    "\n",
    "## One-hot representation\n",
    "\n",
    "In one-hot representation, each feature value is represented by an n-dimensional vector, where n corresponds to the number of possible values the feature can take. In our system, the Token feature can take the value of each token that occurs at least once in the corpus. This feature thus uses a vector with the size of the vocabulary in the corpus. Each possible value is associated with a specific dimension. If this value is represented, that dimension will receive the value 1 and all other dimensions will have the value 0.\n",
    "\n",
    "The system receive a concatenation of all feature representations as input.\n",
    "\n",
    "\n",
    "## What does one-hot look like?\n",
    "\n",
    "We will start with an illustration of a one-hot representation. We will use the capitalization feature for this: it has 6 possible values and is therefore represented by a 6-dimensional vector. If you would like a more precise look, you may consider creating a toy example of a few lines, in which the capitalization feature has different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# create classifier with caps feature only and print vectorizer, then with token only (but you see less)\n",
    "\n",
    "selected_features = ['Cap']\n",
    "\n",
    "feature_values, labels = extract_features_and_gold_labels(trainfile, selected_features)\n",
    "\n",
    "#creating a vectorizing\n",
    "vectorizer = DictVectorizer()\n",
    "#fitting the values to dimensions (creating a mapping) and transforming the current observations according to this mapping\n",
    "capitalization_vectorized = vectorizer.fit_transform(feature_values)\n",
    "print(capitalization_vectorized.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word embeddings\n",
    "\n",
    "We are now going to use word embeddings to represent tokens. We load a pretrained distributional semantic model.\n",
    "You can use the same model as in Exercise 2.1. We tested the exercise with the same model (GoogleNews negative sampling 300 dimensions) as Exercise 2.1 as well.\n",
    "\n",
    "Note: loading the model may take a while. You probably want to run that only once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step takes a while\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('/Users/hernando/Desktop/NLP/Assignment2/NLP_tech_distributional_semantics/models/GoogleNews-vectors-negative300.bin.gz', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dense features...\n",
      "Training classifier....\n",
      "Running evaluation...\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC        754      32    436    259      0       1      3     67    116\n",
      "B-MISC        48     309    124     45      0       2      8     40    126\n",
      "B-ORG        216      47    953    138      0       0      6     67    234\n",
      "B-PER        135      20    273    938      0       1      1    154     95\n",
      "I-LOC          4       7      0      0    131       7     42     38     28\n",
      "I-MISC         1       2      0      5      3      94     22     38     51\n",
      "I-ORG         26       9      4     24     51      16    443    148    114\n",
      "I-PER          0       1      0     11      1       0     18   1084     41\n",
      "O            157     108    283    129     11      44     50    110  37662\n",
      "P: 0.6724982682447125 R: 0.6344319254956768 F1: 0.6517226942239397\n"
     ]
    }
   ],
   "source": [
    "def extract_embeddings_as_features_and_gold(conllfile,word_embedding_model):\n",
    "    '''\n",
    "    Function that extracts features and gold labels using word embeddings\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if len(row) == 6:\n",
    "            if row[0] in word_embedding_model:\n",
    "                vector = word_embedding_model[row[0]]\n",
    "            else:\n",
    "                vector = [0]*300\n",
    "            features.append(vector)\n",
    "            labels.append(row[-1])\n",
    "    return features, labels\n",
    "\n",
    "def create_classifier(features, labels):\n",
    "    '''\n",
    "    Function that creates classifier from features represented as vectors and gold labels\n",
    "    \n",
    "    :param features: list of vector representations of tokens\n",
    "    :param labels: list of gold labels\n",
    "    :type features: list of vectors\n",
    "    :type labels: list of strings\n",
    "    \n",
    "    :returns trained logistic regression classifier\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    lr_classifier = LogisticRegression(solver='saga')\n",
    "    lr_classifier.fit(features, labels)\n",
    "    \n",
    "    return lr_classifier\n",
    "    \n",
    "    \n",
    "def label_data_using_word_embeddings(testfile, word_embedding_model, classifier):\n",
    "    '''\n",
    "    Function that extracts word embeddings as features and gold labels from test data and runs a classifier\n",
    "    \n",
    "    :param testfile: path to test file\n",
    "    :param word_embedding_model: distributional semantic model\n",
    "    :param classifier: trained classifier\n",
    "    :type testfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    :type classifier: LogisticRegression\n",
    "    \n",
    "    :return predictions: list of predicted labels\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    \n",
    "    dense_feature_representations, labels = extract_embeddings_as_features_and_gold(testfile,word_embedding_model)\n",
    "    predictions = classifier.predict(dense_feature_representations)\n",
    "    \n",
    "    return predictions, labels\n",
    "\n",
    "\n",
    "# I printing announcements of where the code is at (since some of these steps take a while)\n",
    "\n",
    "print('Extracting dense features...')\n",
    "dense_feature_representations, labels = extract_embeddings_as_features_and_gold(trainfile,word_embedding_model)\n",
    "print('Training classifier....')\n",
    "classifier = create_classifier(dense_feature_representations, labels)\n",
    "print('Running evaluation...')\n",
    "predicted, gold = label_data_using_word_embeddings(testfile, word_embedding_model, classifier)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predicted, gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including the preceding token\n",
    "\n",
    "We can include the preceding token as a feature in a similar way. We simply concatenate the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dense features...\n",
      "Training classifier...\n",
      "Running evaluation...\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC        754      32    436    259      0       1      3     67    116\n",
      "B-MISC        48     309    124     45      0       2      8     40    126\n",
      "B-ORG        216      47    953    138      0       0      6     67    234\n",
      "B-PER        135      20    273    938      0       1      1    154     95\n",
      "I-LOC          4       7      0      0    131       7     42     38     28\n",
      "I-MISC         1       2      0      5      3      94     22     38     51\n",
      "I-ORG         26       9      4     24     51      16    443    148    114\n",
      "I-PER          0       1      0     11      1       0     18   1084     41\n",
      "O            157     108    283    129     11      44     50    110  37662\n",
      "P: 0.7706433909643632 R: 0.7554149229938387 F1: 0.7620927927958481\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_embeddings_of_current_and_preceding_as_features_and_gold(conllfile,word_embedding_model):\n",
    "    '''\n",
    "    Function that extracts features and gold labels using word embeddings for current and preceding token\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if len(row) == 6:\n",
    "            if row[0] in word_embedding_model:\n",
    "                vector1 = word_embedding_model[row[0]]\n",
    "            else:\n",
    "                vector1 = [0]*300\n",
    "            if row[1] in word_embedding_model:\n",
    "                vector2 = word_embedding_model[row[1]]\n",
    "            else:\n",
    "                vector2 = [0]*300\n",
    "            features.append(np.concatenate((vector1,vector2)))\n",
    "            labels.append(row[-1])\n",
    "    return features, labels\n",
    "    \n",
    "    \n",
    "def label_data_using_word_embeddings_current_and_preceding(testfile, word_embedding_model, classifier):\n",
    "    '''\n",
    "    Function that extracts word embeddings as features (of current and preceding token) and gold labels from test data and runs a trained classifier\n",
    "    \n",
    "    :param testfile: path to test file\n",
    "    :param word_embedding_model: distributional semantic model\n",
    "    :param classifier: trained classifier\n",
    "    :type testfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    :type classifier: LogisticRegression\n",
    "    \n",
    "    :return predictions: list of predicted labels\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    \n",
    "    features, labels = extract_embeddings_of_current_and_preceding_as_features_and_gold(testfile,word_embedding_model)\n",
    "    predictions = classifier.predict(features)\n",
    "    \n",
    "    return predictions, labels\n",
    "\n",
    "\n",
    "\n",
    "print('Extracting dense features...')\n",
    "features, labels = extract_embeddings_of_current_and_preceding_as_features_and_gold(trainfile,word_embedding_model)\n",
    "print('Training classifier...')\n",
    "#we can use the same function as for just the tokens itself\n",
    "classifier = create_classifier(features, labels)\n",
    "print('Running evaluation...')\n",
    "predicted, gold = label_data_using_word_embeddings_current_and_preceding(testfile, word_embedding_model, classifier)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predicted, gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A mixed system\n",
    "\n",
    "The code below combines traditional features with word embeddings. Note that we only include features with a limited range of possible values. Combining one-hot token representations (using highly sparse dimensions) with dense representations is generally not a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Features...\n",
      "Training classifier....\n",
      "Running the evaluation...\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1423      35    150     22      3       2      7      4     22\n",
      "B-MISC        26     515     57     25      0       5      4      0     70\n",
      "B-ORG        234      75   1181     66      1       1     26      3     74\n",
      "B-PER         21      14     55   1462      0       3      2     30     30\n",
      "I-LOC          5       1      2      1    176       8     46      7     11\n",
      "I-MISC         1       5      1      3      4     133     24      2     43\n",
      "I-ORG         18      23     19      3     63      28    558     30     93\n",
      "I-PER          0       9      2     32     10       1     31   1055     16\n",
      "O             19      59     82     46     13      68    107     16  38144\n",
      "P: 0.7718934049584222 R: 0.7858579015591616 F1: 0.7782709946991507\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def extract_word_embedding(token, word_embedding_model):\n",
    "    '''\n",
    "    Function that returns the word embedding for a given token out of a distributional semantic model and a 300-dimension vector of 0s otherwise\n",
    "    \n",
    "    :param token: the token\n",
    "    :param word_embedding_model: the distributional semantic model\n",
    "    :type token: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :returns a vector representation of the token\n",
    "    '''\n",
    "    if token in word_embedding_model:\n",
    "        vector = word_embedding_model[token]\n",
    "    else:\n",
    "        vector = [0]*300\n",
    "    return vector\n",
    "\n",
    "\n",
    "def extract_feature_values(row, selected_features):\n",
    "    '''\n",
    "    Function that extracts feature value pairs from row\n",
    "    \n",
    "    :param row: row from conll file\n",
    "    :param selected_features: list of selected features\n",
    "    :type row: string\n",
    "    :type selected_features: list of strings\n",
    "    \n",
    "    :returns: dictionary of feature value pairs\n",
    "    '''\n",
    "    feature_values = {}\n",
    "    for feature_name in selected_features:\n",
    "        r_index = feature_to_index.get(feature_name)\n",
    "        feature_values[feature_name] = row[r_index]\n",
    "        \n",
    "    return feature_values\n",
    "    \n",
    "    \n",
    "def create_vectorizer_traditional_features(feature_values):\n",
    "    '''\n",
    "    Function that creates vectorizer for set of feature values\n",
    "    \n",
    "    :param feature_values: list of dictionaries containing feature-value pairs\n",
    "    :type feature_values: list of dictionairies (key and values are strings)\n",
    "    \n",
    "    :returns: vectorizer with feature values fitted\n",
    "    '''\n",
    "    vectorizer = DictVectorizer()\n",
    "    vectorizer.fit(feature_values)\n",
    "    \n",
    "    return vectorizer\n",
    "        \n",
    "    \n",
    "def combine_sparse_and_dense_features(dense_vectors, sparse_features):\n",
    "    '''\n",
    "    Function that takes sparse and dense feature representations and appends their vector representation\n",
    "    \n",
    "    :param dense_vectors: list of dense vector representations\n",
    "    :param sparse_features: list of sparse vector representations\n",
    "    :type dense_vector: list of arrays\n",
    "    :type sparse_features: list of lists\n",
    "    \n",
    "    :returns: list of arrays in which sparse and dense vectors are concatenated\n",
    "    '''\n",
    "    \n",
    "    combined_vectors = []\n",
    "    sparse_vectors = np.array(sparse_features.toarray())\n",
    "    \n",
    "    for index, vector in enumerate(sparse_vectors):\n",
    "        combined_vector = np.concatenate((vector,dense_vectors[index]))\n",
    "        combined_vectors.append(combined_vector)\n",
    "    return combined_vectors\n",
    "    \n",
    "\n",
    "def extract_traditional_features_and_embeddings_plus_gold_labels(conllfile, word_embedding_model, vectorizer=None):\n",
    "    '''\n",
    "    Function that extracts traditional features as well as embeddings and gold labels using word embeddings for current and preceding token\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    dense_vectors = []\n",
    "    traditional_features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if len(row) == 6:\n",
    "            token_vector = extract_word_embedding(row[0], word_embedding_model)\n",
    "            pt_vector = extract_word_embedding(row[1], word_embedding_model)\n",
    "            dense_vectors.append(np.concatenate((token_vector,pt_vector)))\n",
    "            #mixing very sparse representations (for one-hot tokens) and dense representations is a bad idea\n",
    "            #we thus only use other features with limited values\n",
    "            other_features = extract_feature_values(row, ['Cap','Pos','Chunklabel'])\n",
    "            traditional_features.append(other_features)\n",
    "            #adding gold label to labels\n",
    "            labels.append(row[-1])\n",
    "            \n",
    "    #create vector representation of traditional features\n",
    "    if vectorizer is None:\n",
    "        #creates vectorizer that provides mapping (only if not created earlier)\n",
    "        vectorizer = create_vectorizer_traditional_features(traditional_features)\n",
    "    sparse_features = vectorizer.transform(traditional_features)\n",
    "    combined_vectors = combine_sparse_and_dense_features(dense_vectors, sparse_features)\n",
    "    \n",
    "    return combined_vectors, vectorizer, labels\n",
    "\n",
    "def label_data_with_combined_features(testfile, classifier, vectorizer, word_embedding_model):\n",
    "    '''\n",
    "    Function that labels data with model using both sparse and dense features\n",
    "    '''\n",
    "    feature_vectors, vectorizer, goldlabels = extract_traditional_features_and_embeddings_plus_gold_labels(testfile, word_embedding_model, vectorizer)\n",
    "    predictions = classifier.predict(feature_vectors)\n",
    "    \n",
    "    return predictions, goldlabels\n",
    "\n",
    "\n",
    "print('Extracting Features...')\n",
    "feature_vectors, vectorizer, gold_labels = extract_traditional_features_and_embeddings_plus_gold_labels(trainfile, word_embedding_model)\n",
    "print('Training classifier....')\n",
    "lr_classifier = create_classifier(feature_vectors, gold_labels)\n",
    "print('Running the evaluation...')\n",
    "predictions, goldlabels = label_data_with_combined_features(testfile, lr_classifier, vectorizer, word_embedding_model)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
