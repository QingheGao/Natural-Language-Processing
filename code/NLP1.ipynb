{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import codecs\n",
    "import glob\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "import nltk\n",
    "import text_normalizer\n",
    "from afinn import Afinn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import re\n",
    "#https://stackabuse.com/removing-stop-words-from-strings-in-python/\n",
    "#stopword\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1=pd.read_csv(('/Users/hernando/Desktop/NLP/NLP_tech_module1/vaccination_stance_annotations.csv'), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_2=pd.read_csv(('/Users/hernando/Desktop/NLP/NLP_tech_module1/student2.csv'), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "student1_result=[i for i in student_1['stance']]\n",
    "student2_result=[i for i in student_2['stance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22248803827751196"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kappa=sklearn.metrics.cohen_kappa_score(student1_result,student2_result)\n",
    "kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confuse(predictions,y_test,n):\n",
    "    confuse=np.zeros((n,n))\n",
    "    k=z=g=h=f=j=y=q=o=0\n",
    "    for i in range(int(len(predictions))):\n",
    "        if y_test[i]=='F':\n",
    "            if predictions[i]=='F':\n",
    "                k+=1\n",
    "            elif predictions[i]=='N':\n",
    "                z+=1\n",
    "            else:\n",
    "                g+=1\n",
    "\n",
    "        elif y_test[i]=='N':\n",
    "            if predictions[i]=='F':\n",
    "                h+=1\n",
    "            elif predictions[i]=='N':\n",
    "                f+=1\n",
    "            else:\n",
    "                j+=1\n",
    "        else:\n",
    "            if predictions[i]=='F':\n",
    "                y+=1\n",
    "            elif predictions[i]=='N':\n",
    "                q+=1\n",
    "            else:\n",
    "                o+=1\n",
    "                \n",
    "        \n",
    "    confuse[0,0]=k\n",
    "    confuse[0,1]=z\n",
    "    confuse[0,2]=g\n",
    "    confuse[1,0]=h\n",
    "    confuse[1,1]=f\n",
    "    confuse[1,2]=j\n",
    "    confuse[2,0]=y\n",
    "    confuse[2,1]=q\n",
    "    confuse[2,2]=o\n",
    "    return confuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 2., 4.],\n",
       "       [1., 3., 3.],\n",
       "       [2., 1., 4.]])"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confuse(student1_result,student2_result,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['Number','Attitute','Content','Score']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Number']=np.arange(0,2000,1)\n",
    "df.loc[np.arange(0,1000,1),'Attitute']=0\n",
    "df.loc[np.arange(1000,2000,1),'Attitute']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.getcwd()\n",
    "os.chdir('/Users/hernando/Desktop/NLP/NLP_tech_module1/SentimentExperiment/review_polarity_data/neg')\n",
    "\n",
    "def txtcombine(df):\n",
    "    \n",
    "    files = glob.glob('*.txt')\n",
    "    z=0\n",
    "    for filename in files:\n",
    "        kkk=open(filename,'r')\n",
    "        url=kkk.read().replace('\\n','') \n",
    "        df.loc[z, 'Content'] = url\n",
    "        kkk.close()\n",
    "        z=z+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtcombine(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir('/Users/hernando/Desktop/NLP/NLP_tech_module1/SentimentExperiment/review_polarity_data/pos')\n",
    "\n",
    "def txtcombine_2(df):\n",
    "    \n",
    "    files = glob.glob('*.txt')\n",
    "    z=1000\n",
    "    for filename in files:\n",
    "        kkk=open(filename,'r')\n",
    "        url=kkk.read().replace('\\n','') \n",
    "        df.loc[z, 'Content'] = url\n",
    "        kkk.close()\n",
    "        z=z+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtcombine_2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Attitute</th>\n",
       "      <th>Content</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bad . bad . bad . that one word seems to prett...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>isn't it the ultimate sign of a movie's cinema...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>\" gordy \" is not a movie , it is a 90-minute-...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>disconnect the phone line . don't accept the c...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>when robert forster found himself famous again...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>this is my first review that i post to this ne...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>\" lake placid \" marks yet another entry in th...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>the main problem with martin lawrence's pet pr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>\" with all that education , you should know w...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>jet li busted onto the american action movie s...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>starring shawnee smith ; donovan leitch ; rick...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>in 1970s , many european intellectuals , espec...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>the army comedy genre has never turned out a t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>and just when you thought joblo was getting a ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>talk about beating a dead horse ! when home al...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>capsule : godawful \" comedy \" that's amazingly...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>in our time . in our modern world , where the ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>about an hour or so into \" the jackal , \" a ch...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>i'm not sure who the genius is who came up wit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1989's \" major league \" was a delightful surpr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>late in down to you , the lead female characte...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>capsule : annoyingly unentertaining , obvious ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>warner brothers has scored another marketing c...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>the tagline on random hearts reads \" in a perf...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>walken stars as a mobster who is kidnapped and...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>post-chasing amy , a slew of love-triangle mov...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>underwater science fiction stays submerged sph...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>\" the animal \" is a marginally inspired comed...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>the general's daughter is a heartless , absurd...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>brian de palma's snake eyes stars nicolas cage...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>today , war became a reality to me after seein...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>1971</td>\n",
       "      <td>1</td>\n",
       "      <td>mickey mouse had better watch his back -- ther...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>1972</td>\n",
       "      <td>1</td>\n",
       "      <td>scarface , a remake of the 1932 film of the sa...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>1973</td>\n",
       "      <td>1</td>\n",
       "      <td>\" six days , seven nights \" is a summer movie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>1974</td>\n",
       "      <td>1</td>\n",
       "      <td>the premise of the new teen-targeted horror fi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>1975</td>\n",
       "      <td>1</td>\n",
       "      <td>&gt;from the commercials , this looks like a mild...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>1976</td>\n",
       "      <td>1</td>\n",
       "      <td>i am starting to write this review before goin...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>1977</td>\n",
       "      <td>1</td>\n",
       "      <td>losing a job is not an all too uncommon thing ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>1978</td>\n",
       "      <td>1</td>\n",
       "      <td>jake kasdan , son of one of the best screenwri...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>1979</td>\n",
       "      <td>1</td>\n",
       "      <td>melvin udall is a heartless man . he spends hi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>1980</td>\n",
       "      <td>1</td>\n",
       "      <td>in roger michell's romantic comedy notting hil...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>1981</td>\n",
       "      <td>1</td>\n",
       "      <td>a group of high school kids mix up with a grou...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>1982</td>\n",
       "      <td>1</td>\n",
       "      <td>kevin smith is like a big kid . his humor is t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>1983</td>\n",
       "      <td>1</td>\n",
       "      <td>what a great film . what a stunning , touching...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>1984</td>\n",
       "      <td>1</td>\n",
       "      <td>if there's one thing in common about all of ho...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>1985</td>\n",
       "      <td>1</td>\n",
       "      <td>i saw simon birch in a basically sold out thea...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>1986</td>\n",
       "      <td>1</td>\n",
       "      <td>gothic murder-mystery yarns are not a new conc...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>1987</td>\n",
       "      <td>1</td>\n",
       "      <td>with more and more television shows having gay...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>1988</td>\n",
       "      <td>1</td>\n",
       "      <td>capsule : the best place to start if you're a ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>1989</td>\n",
       "      <td>1</td>\n",
       "      <td>this summer , one of the most racially charged...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>satirical films usually fall into one of two c...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>1991</td>\n",
       "      <td>1</td>\n",
       "      <td>i know that \" funnest \" isn't a word .  \" fun ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>curdled is a deliciously dark and witty black ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>1993</td>\n",
       "      <td>1</td>\n",
       "      <td>as fairy tales go , cinderella has to be one o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>in wonder boys michael douglas plays an aged w...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>1</td>\n",
       "      <td>one of the funniest carry on movies and the th...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>1</td>\n",
       "      <td>i remember making a pact , right after `patch ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>1</td>\n",
       "      <td>barely scrapping by playing at a nyc piano bar...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>1</td>\n",
       "      <td>if the current trends of hollywood filmmaking ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>capsule : the director of cure brings a weird ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Number  Attitute                                            Content  \\\n",
       "0          0         0  bad . bad . bad . that one word seems to prett...   \n",
       "1          1         0  isn't it the ultimate sign of a movie's cinema...   \n",
       "2          2         0   \" gordy \" is not a movie , it is a 90-minute-...   \n",
       "3          3         0  disconnect the phone line . don't accept the c...   \n",
       "4          4         0  when robert forster found himself famous again...   \n",
       "5          5         0  this is my first review that i post to this ne...   \n",
       "6          6         0   \" lake placid \" marks yet another entry in th...   \n",
       "7          7         0  the main problem with martin lawrence's pet pr...   \n",
       "8          8         0   \" with all that education , you should know w...   \n",
       "9          9         0  jet li busted onto the american action movie s...   \n",
       "10        10         0  starring shawnee smith ; donovan leitch ; rick...   \n",
       "11        11         0  in 1970s , many european intellectuals , espec...   \n",
       "12        12         0  the army comedy genre has never turned out a t...   \n",
       "13        13         0  and just when you thought joblo was getting a ...   \n",
       "14        14         0  talk about beating a dead horse ! when home al...   \n",
       "15        15         0  capsule : godawful \" comedy \" that's amazingly...   \n",
       "16        16         0  in our time . in our modern world , where the ...   \n",
       "17        17         0  about an hour or so into \" the jackal , \" a ch...   \n",
       "18        18         0  i'm not sure who the genius is who came up wit...   \n",
       "19        19         0  1989's \" major league \" was a delightful surpr...   \n",
       "20        20         0  late in down to you , the lead female characte...   \n",
       "21        21         0  capsule : annoyingly unentertaining , obvious ...   \n",
       "22        22         0  warner brothers has scored another marketing c...   \n",
       "23        23         0  the tagline on random hearts reads \" in a perf...   \n",
       "24        24         0  walken stars as a mobster who is kidnapped and...   \n",
       "25        25         0  post-chasing amy , a slew of love-triangle mov...   \n",
       "26        26         0  underwater science fiction stays submerged sph...   \n",
       "27        27         0   \" the animal \" is a marginally inspired comed...   \n",
       "28        28         0  the general's daughter is a heartless , absurd...   \n",
       "29        29         0  brian de palma's snake eyes stars nicolas cage...   \n",
       "...      ...       ...                                                ...   \n",
       "1970    1970         1  today , war became a reality to me after seein...   \n",
       "1971    1971         1  mickey mouse had better watch his back -- ther...   \n",
       "1972    1972         1  scarface , a remake of the 1932 film of the sa...   \n",
       "1973    1973         1   \" six days , seven nights \" is a summer movie...   \n",
       "1974    1974         1  the premise of the new teen-targeted horror fi...   \n",
       "1975    1975         1  >from the commercials , this looks like a mild...   \n",
       "1976    1976         1  i am starting to write this review before goin...   \n",
       "1977    1977         1  losing a job is not an all too uncommon thing ...   \n",
       "1978    1978         1  jake kasdan , son of one of the best screenwri...   \n",
       "1979    1979         1  melvin udall is a heartless man . he spends hi...   \n",
       "1980    1980         1  in roger michell's romantic comedy notting hil...   \n",
       "1981    1981         1  a group of high school kids mix up with a grou...   \n",
       "1982    1982         1  kevin smith is like a big kid . his humor is t...   \n",
       "1983    1983         1  what a great film . what a stunning , touching...   \n",
       "1984    1984         1  if there's one thing in common about all of ho...   \n",
       "1985    1985         1  i saw simon birch in a basically sold out thea...   \n",
       "1986    1986         1  gothic murder-mystery yarns are not a new conc...   \n",
       "1987    1987         1  with more and more television shows having gay...   \n",
       "1988    1988         1  capsule : the best place to start if you're a ...   \n",
       "1989    1989         1  this summer , one of the most racially charged...   \n",
       "1990    1990         1  satirical films usually fall into one of two c...   \n",
       "1991    1991         1  i know that \" funnest \" isn't a word .  \" fun ...   \n",
       "1992    1992         1  curdled is a deliciously dark and witty black ...   \n",
       "1993    1993         1  as fairy tales go , cinderella has to be one o...   \n",
       "1994    1994         1  in wonder boys michael douglas plays an aged w...   \n",
       "1995    1995         1  one of the funniest carry on movies and the th...   \n",
       "1996    1996         1  i remember making a pact , right after `patch ...   \n",
       "1997    1997         1  barely scrapping by playing at a nyc piano bar...   \n",
       "1998    1998         1  if the current trends of hollywood filmmaking ...   \n",
       "1999    1999         1  capsule : the director of cure brings a weird ...   \n",
       "\n",
       "     Score  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3      NaN  \n",
       "4      NaN  \n",
       "5      NaN  \n",
       "6      NaN  \n",
       "7      NaN  \n",
       "8      NaN  \n",
       "9      NaN  \n",
       "10     NaN  \n",
       "11     NaN  \n",
       "12     NaN  \n",
       "13     NaN  \n",
       "14     NaN  \n",
       "15     NaN  \n",
       "16     NaN  \n",
       "17     NaN  \n",
       "18     NaN  \n",
       "19     NaN  \n",
       "20     NaN  \n",
       "21     NaN  \n",
       "22     NaN  \n",
       "23     NaN  \n",
       "24     NaN  \n",
       "25     NaN  \n",
       "26     NaN  \n",
       "27     NaN  \n",
       "28     NaN  \n",
       "29     NaN  \n",
       "...    ...  \n",
       "1970   NaN  \n",
       "1971   NaN  \n",
       "1972   NaN  \n",
       "1973   NaN  \n",
       "1974   NaN  \n",
       "1975   NaN  \n",
       "1976   NaN  \n",
       "1977   NaN  \n",
       "1978   NaN  \n",
       "1979   NaN  \n",
       "1980   NaN  \n",
       "1981   NaN  \n",
       "1982   NaN  \n",
       "1983   NaN  \n",
       "1984   NaN  \n",
       "1985   NaN  \n",
       "1986   NaN  \n",
       "1987   NaN  \n",
       "1988   NaN  \n",
       "1989   NaN  \n",
       "1990   NaN  \n",
       "1991   NaN  \n",
       "1992   NaN  \n",
       "1993   NaN  \n",
       "1994   NaN  \n",
       "1995   NaN  \n",
       "1996   NaN  \n",
       "1997   NaN  \n",
       "1998   NaN  \n",
       "1999   NaN  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.Content.values, df.Attitute.values,\n",
    "                                                    stratify=df.Attitute.values, test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon=pd.read_csv('/Users/hernando/Desktop/NLP/NLP_tech_module1/SentimentExperiment/SentiLexiconFromSentiWordNet.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon['SynsetTerms']=lexicon['SynsetTerms'].str.replace('#','')\n",
    "lexicon['SynsetTerms']=lexicon['SynsetTerms'].str.replace('0','')\n",
    "lexicon['SynsetTerms']=lexicon['SynsetTerms'].str.replace('1','')\n",
    "lexicon['SynsetTerms']=lexicon['SynsetTerms'].str.replace('2','')\n",
    "lexicon['SynsetTerms']=lexicon['SynsetTerms'].str.replace('3','')\n",
    "lexicon['SynsetTerms']=lexicon['SynsetTerms'].str.replace('4','')\n",
    "lexicon['SynsetTerms']=lexicon['SynsetTerms'].str.replace('5','')\n",
    "lexicon['SynsetTerms']=lexicon['SynsetTerms'].str.replace('6','')\n",
    "lexicon['SynsetTerms']=lexicon['SynsetTerms'].str.replace('7','')\n",
    "lexicon['SynsetTerms']=lexicon['SynsetTerms'].str.replace('8','')\n",
    "lexicon['SynsetTerms']=lexicon['SynsetTerms'].str.replace('9','')\n",
    "# lexicon['SynsetTerms']=lexicon['SynsetTerms'].str.replace(' ',',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hernando/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for i in range(29095):\n",
    "    str = lexicon['SynsetTerms'][i]\n",
    "    lexicon['SynsetTerms'][i] = str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.reset_index()\n",
    "df_test=df_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea from https://stackoverflow.com/a/47091490\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "def text_split_1(df):\n",
    "    df_1=decontracted(df)\n",
    "    string_list=['.',',','?',')','\"','--','(',':',';','%','>','<', '^','&','/','*','&','%','$','#','@','!','~']\n",
    "    list_3=nltk.word_tokenize(df_1)\n",
    "    for i in string_list:\n",
    "        while i in list_3:\n",
    "            list_3.remove(i)\n",
    "#     tokens_without_sw = [word for word in list_3 if not word in stopwords.words()]\n",
    "#     list_final=nltk.pos_tag(tokens_without_sw)\n",
    "    return list_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(df,lexicon):\n",
    "    outcome=np.zeros((len(df)))\n",
    "    for i in range(len(df)):\n",
    "        print(i)\n",
    "        begin=text_split_1(df[i])\n",
    "        score=0\n",
    "        positive=negative=0\n",
    "        count=0\n",
    "        for h in begin:\n",
    "            for k,u in enumerate(lexicon['SynsetTerms']):\n",
    "                if h in u:\n",
    "                    score=score+lexicon['PosScore'][k]-lexicon['NegScore'][k]\n",
    "                    count+=1\n",
    "        score= score/count\n",
    "        if score>0:\n",
    "            outcome[i]=1\n",
    "        else:\n",
    "            outcome[i]=0            \n",
    "    return outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n"
     ]
    }
   ],
   "source": [
    "outcome_1=score(X_test,lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6901669758812615, 0.5825)"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(outcome_1,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from this course\n",
    "\n",
    "\n",
    "def create_vectorizer_and_classifier(reviews, labels):\n",
    "    '''\n",
    "    This function creates a vectorizer and logistic regression classifer with bag-of-word features\n",
    "    \n",
    "    :param reviews: the review texts (scenario sentiment classification on reviews)\n",
    "    :param labels: the gold labels for the texts\n",
    "    :type reviews: list of strings (each element is the text of a review)\n",
    "    :type labels: list of strings (each element provides the gold label for the corresponding review)\n",
    "    \n",
    "    :returns: the vectorizer that provides the mapping from tokens to a vector and a trained classifier\n",
    "    '''\n",
    "    \n",
    "    vectorizer = CountVectorizer(min_df=1,tokenizer=word_tokenize)\n",
    "    training_vector = vectorizer.fit_transform(reviews)\n",
    "    myclassifier = LogisticRegression()\n",
    "    myclassifier.fit(training_vector, labels)\n",
    "    \n",
    "    return vectorizer, myclassifier\n",
    "def classify_data(evaluation_reviews,vectorizer, sentiment_classifier):\n",
    "    '''\n",
    "    This function takes a list of strings and labels them with a classifier\n",
    "    \n",
    "    :param evaluation_reviews: the reviews to be used for evaluation\n",
    "    :param vectorizer: the vectorizer to turn text in bag-of-word vector representations\n",
    "    :param sentiment_classifier: a trained classifier\n",
    "    :type evaluation_reviews: list of strings\n",
    "    :type vectorizer: CountVectorizer\n",
    "    :type classifier: LogisticRegression() (can be another type of classifier as well)\n",
    "    \n",
    "    :returns: a list of predicted labels\n",
    "    '''\n",
    "\n",
    "    evaluation_vector = vectorizer.transform(evaluation_reviews)\n",
    "    predictions = sentiment_classifier.predict(evaluation_vector)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.Content.values, df.Attitute.values,\n",
    "                                                    stratify=df.Attitute.values, test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer, myclassifier=create_vectorizer_and_classifier(X_train, y_train)\n",
    "predictions=classify_data(X_test,vectorizer, myclassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accuracy and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(predictions,y_test):\n",
    "    confuse=np.zeros((2,2))\n",
    "    k=z=g=h=0\n",
    "    for i in range(int(len(predictions))):\n",
    "        if y_test[i]==0:\n",
    "            if predictions[i]==0:\n",
    "                k+=1\n",
    "            else:\n",
    "                z+=1\n",
    "\n",
    "        else:\n",
    "            if predictions[i]==0:\n",
    "                g+=1\n",
    "            else:\n",
    "                h+=1\n",
    "        \n",
    "    confuse[0,0]=k\n",
    "    confuse[0,1]=z\n",
    "    confuse[1,0]=g\n",
    "    confuse[1,1]=h\n",
    "    \n",
    "    accuracy= (k+h)/(k+z+g+h)\n",
    "    precision=h/(z+h)\n",
    "    recall=h/(g+h)\n",
    "    f1_1=2*precision*recall/(recall+precision)\n",
    "    print('accuracy, f1:',accuracy,f1_1)\n",
    "    return f1_1,accuracy   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy, f1: 0.5825 0.6901669758812615\n"
     ]
    }
   ],
   "source": [
    "f1_lexicon,accuracy_lexicon=f1(outcome_1,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy, f1: 0.81 0.8090452261306532\n"
     ]
    }
   ],
   "source": [
    "f1_machine,accuracy_machine=f1(predictions,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore, pval = proportions_ztest([accuracy_lexicon*400, accuracy_machine*400],[400, 400], alternative='two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.631965209123369e-12"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8305935501405528e-152"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "scipy.stats.binom_test(0.81, n=400, p=0.5825)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
